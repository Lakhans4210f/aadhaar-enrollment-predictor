# ML MODEL TRAINING & EXPORT SCRIPT
# Compatible with your cleaned AADHAAR dataset

import pandas as pd
import numpy as np
import joblib
import json
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

print('\n' + '='*70)
print('ML MODEL TRAINING: COMPATIBLE WITH CLEANED AADHAAR DATA')
print('='*70)

# STEP 1: LOAD YOUR CLEANED DATASET
print('\n[STEP 1] Loading cleaned dataset...')
try:
    df = pd.read_csv('cleaned_aadhaar_biometric_processed.csv')
    print(f'  ✓ Loaded: {df.shape[0]:,} records, {df.shape[1]} columns')
except:
    print('  ✗ Error: cleaned_aadhaar_biometric_processed.csv not found')
    raise

print(f'  Columns: {list(df.columns)}')
print(f'  Data types:\n{df.dtypes}')

# STEP 2: PREPARE FEATURES
print('\n[STEP 2] Preparing features for ML models...')

feature_cols = []
if 'bio_age_5_17' in df.columns:
    feature_cols.append('bio_age_5_17')
if 'bio_age_17_' in df.columns:
    feature_cols.append('bio_age_17_')
if 'is_outlier' in df.columns:
    feature_cols.append('is_outlier')

# Create target variable
if 'total_bio' not in df.columns:
    df['total_bio'] = df['bio_age_5_17'] + df['bio_age_17_']

y = df['total_bio']
X = df[feature_cols].fillna(0)

print(f'  Features: {feature_cols}')
print(f'  Target: total_bio')
print(f'  X shape: {X.shape}, y shape: {y.shape}')

# STEP 3: SPLIT DATA
print('\n[STEP 3] Train-test split...')
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(f'  Train: {X_train.shape[0]:,} | Test: {X_test.shape[0]:,}')

# STEP 4: SCALE FEATURES
print('\n[STEP 4] Feature scaling...')
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print('  ✓ Scaled')

# STEP 5: TRAIN MODELS
print('\n' + '='*70)
print('TRAINING 3 ML MODELS')
print('='*70)

models = {}
results = {}

# Model 1: Linear Regression
print('\n[MODEL 1] Linear Regression')
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)
mae_lr = mean_absolute_error(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
r2_lr = r2_score(y_test, y_pred_lr)
models['linear_regression'] = lr
results['linear_regression'] = {'MAE': mae_lr, 'RMSE': rmse_lr, 'R2': r2_lr}
print(f'  MAE: {mae_lr:.4f} | RMSE: {rmse_lr:.4f} | R²: {r2_lr:.4f}')

# Model 2: Random Forest
print('\n[MODEL 2] Random Forest')
rf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42, n_jobs=-1)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)
models['random_forest'] = rf
results['random_forest'] = {'MAE': mae_rf, 'RMSE': rmse_rf, 'R2': r2_rf}
print(f'  MAE: {mae_rf:.4f} | RMSE: {rmse_rf:.4f} | R²: {r2_rf:.4f}')

# Model 3: Gradient Boosting
print('\n[MODEL 3] Gradient Boosting')
gb = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
mae_gb = mean_absolute_error(y_test, y_pred_gb)
rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))
r2_gb = r2_score(y_test, y_pred_gb)
models['gradient_boosting'] = gb
results['gradient_boosting'] = {'MAE': mae_gb, 'RMSE': rmse_gb, 'R2': r2_gb}
print(f'  MAE: {mae_gb:.4f} | RMSE: {rmse_gb:.4f} | R²: {r2_gb:.4f}')

# STEP 6: FIND BEST MODEL
print('\n' + '='*70)
print('MODEL COMPARISON')
print('='*70)
best_model_name = max(results.keys(), key=lambda x: results[x]['R2'])
print(f'\n✓ BEST MODEL: {best_model_name.upper()}')
for name, metrics in results.items():
    print(f'  {name}: R²={metrics["R2"]:.4f}')

# STEP 7: EXPORT MODELS
print('\n' + '='*70)
print('EXPORTING MODELS')
print('='*70)

for model_name, model in models.items():
    filename = f'{model_name}_model.pkl'
    joblib.dump(model, filename)
    print(f'  ✓ Saved: {filename}')

joblib.dump(scaler, 'scaler.pkl')
print('  ✓ Saved: scaler.pkl')

metadata = {
    'feature_columns': feature_cols,
    'model_performance': results,
    'best_model': best_model_name,
    'dataset_rows': len(df),
    'training_date': str(pd.Timestamp.now())
}
with open('model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
print('  ✓ Saved: model_metadata.json')

print('\n✅ ALL MODELS TRAINED & EXPORTED!')
print('\nNext: Download .pkl files and update streamlit_app.py')
